{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c10bc3e",
   "metadata": {},
   "source": [
    "## M161 first question notebook, prepare test data\n",
    "## Data preprocessing\n",
    "### Data cleaning I\n",
    " 1. check types \n",
    " 2. check for null values\n",
    " 3. check duplicates\n",
    " 4. keeping 10000 instances to reduce computation load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dee38ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "First 5 rows of the dataset:\n",
      "       Id                                              Title  \\\n",
      "0  262120  Tracy Morgan upgraded to fair condition after ...   \n",
      "1  175132  Smartphones Weigh on Samsung Electronics as Gu...   \n",
      "2  218739  FBI denies fumbling testimony on 'X-Men' direc...   \n",
      "3  253483  Bachelorette 2014 Spoilers: Week 3 Recap ??? E...   \n",
      "4  224109  Barack Obama honours Frankie Knuckles in lette...   \n",
      "\n",
      "                                             Content  \n",
      "0   actor and comedian tracy morgan has been upgr...  \n",
      "1  samsung electronics co ltd on tuesday issued u...  \n",
      "2   michael f. egan iii said in a press conferenc...  \n",
      "3   i am having mixed emotions for what is about ...  \n",
      "4   u.s. president barack obama has paid a specia...  \n",
      "\n",
      "Data summary:\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 47912 entries, 0 to 47911\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   Id       47912 non-null  int64\n",
      " 1   Title    47912 non-null  str  \n",
      " 2   Content  47912 non-null  str  \n",
      "dtypes: int64(1), str(2)\n",
      "memory usage: 1.1 MB\n",
      "None\n",
      "\n",
      "Missing values in each column:\n",
      "Id         0\n",
      "Title      0\n",
      "Content    0\n",
      "dtype: int64\n",
      "\n",
      "Column data types:\n",
      "Id         int64\n",
      "Title        str\n",
      "Content      str\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = 'bigdata2025classification/test_without_labels.csv'\n",
    "\n",
    "def load_and_process_data(file_path):\n",
    "    # Load data from a CSV file\n",
    "    dataTrain = pd.read_csv(file_path)\n",
    "\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(\"First 5 rows of the dataset:\")\n",
    "    print(dataTrain.head())\n",
    "\n",
    "    print(\"\\nData summary:\")\n",
    "    print(dataTrain.info())\n",
    "\n",
    "    # Check for missing values in the dataframe\n",
    "    print(\"\\nMissing values in each column:\")\n",
    "    print(dataTrain.isnull().sum())\n",
    "    \n",
    "    return dataTrain\n",
    "\n",
    "dataTrain = load_and_process_data(file_path)\n",
    "\n",
    "# check column data types\n",
    "def check_column_types(dataTrain):\n",
    "    print(\"\\nColumn data types:\")\n",
    "    print(dataTrain.dtypes)\n",
    "\n",
    "check_column_types(dataTrain)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2ea57",
   "metadata": {},
   "source": [
    "## Data cleaning II continue\n",
    "3. check for duplicates\n",
    "***************************\n",
    "### note\n",
    "- the data types of all exeprt Id column is \"object\" in pandas, it works, but could be converted to String for a performance uplift.\n",
    "*****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4de965e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 0\n",
      "\n",
      "Number of duplicate rows based on Title: 251\n",
      "\n",
      "Number of duplicate rows based on Content: 419\n",
      "\n",
      "Number of duplicate rows based on Title and Content: 111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(111)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicate rows in the dataframe\n",
    "def check_duplicates(dataTrain):\n",
    "    duplicate_count = dataTrain.duplicated().sum()\n",
    "    print(f\"\\nNumber of duplicate rows: {duplicate_count}\")\n",
    "    return duplicate_count\n",
    "\n",
    "check_duplicates(dataTrain)\n",
    "\n",
    "# Check for duplicates based only on 'Title' column\n",
    "def check_title_duplicates(dataTrain):\n",
    "    if 'Title' in dataTrain.columns:\n",
    "        dup_count = dataTrain.duplicated(subset=['Title']).sum()\n",
    "        print(f\"\\nNumber of duplicate rows based on Title: {dup_count}\")\n",
    "        return dup_count\n",
    "    else:\n",
    "        print(\"'Title' column not found in the dataframe.\")\n",
    "        return None\n",
    "\n",
    "check_title_duplicates(dataTrain)\n",
    "\n",
    "# Check for duplicates based only on 'Content' column\n",
    "def check_content_duplicates(dataTrain):\n",
    "    if 'Content' in dataTrain.columns:\n",
    "        dup_count = dataTrain.duplicated(subset=['Content']).sum()\n",
    "        print(f\"\\nNumber of duplicate rows based on Content: {dup_count}\")\n",
    "        return dup_count\n",
    "    else:\n",
    "        print(\"'Content' column not found in the dataframe.\")\n",
    "        return None\n",
    "\n",
    "check_content_duplicates(dataTrain)\n",
    "# Check for duplicates based on 'Title' and 'Content' columns\n",
    "def check_title_content_duplicates(dataTrain):\n",
    "    if 'Title' in dataTrain.columns and 'Content' in dataTrain.columns:\n",
    "        dup_count = dataTrain.duplicated(subset=['Title', 'Content']).sum()\n",
    "        print(f\"\\nNumber of duplicate rows based on Title and Content: {dup_count}\")\n",
    "        return dup_count\n",
    "    else:\n",
    "        print(\"'Title' and/or 'Content' columns not found in the dataframe.\")\n",
    "        return None\n",
    "\n",
    "check_title_content_duplicates(dataTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479bc5bd",
   "metadata": {},
   "source": [
    "## Duplicates not removed from test case (it makes no sense for evaluation later)\n",
    "*****************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ebb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove duplicates based on 'Title' and 'Content' columns, keeping the first occurrence\n",
    "# dataTrain = dataTrain.drop_duplicates(subset=['Title', 'Content'], keep='first')\n",
    "# print(\"\\nDuplicates based on Title and Content removed. Data shape:\", dataTrain.shape)\n",
    "\n",
    "\n",
    "# # Reset the index after removing duplicates\n",
    "# dataTrain = dataTrain.reset_index(drop=True)\n",
    "# print(\"\\nIndex reset. Data shape:\", dataTrain.shape)\n",
    "# dataTrain.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2df3b7",
   "metadata": {},
   "source": [
    "\n",
    "### Remove words not in English dictionary\n",
    "\n",
    "- **probably could change dictionary for better results but it works...**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0efce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Download the words corpus if not already present\n",
    "nltk.download('words')\n",
    "english_words = set(words.words())\n",
    "\n",
    "def clean_text(text):\n",
    "    # Split text into words\n",
    "    word_list = re.findall(r'\\b\\w+\\b', str(text))\n",
    "    cleaned_words = []\n",
    "    for word in word_list:\n",
    "        # Drop any word not in dictionary\n",
    "        if word.lower() not in english_words:\n",
    "            continue\n",
    "        # Drop words with 2+ repeating chars not in dictionary (redundant now, but kept for clarity)\n",
    "        if re.search(r'(.)\\1{1,}', word):\n",
    "            if word.lower() not in english_words:\n",
    "                continue\n",
    "        cleaned_words.append(word)\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Apply to both columns\n",
    "dataTrain['Title'] = dataTrain['Title'].apply(clean_text)\n",
    "dataTrain['Content'] = dataTrain['Content'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da1ad2",
   "metadata": {},
   "source": [
    "## Text clean up \n",
    "1. Expand contractions\n",
    "2. Convert to lowercase\n",
    "3. Remove special characters (keep only letters and spaces)\n",
    "4. Remove extra spaces\n",
    "5. Remove stopwords, lemmatize, and stem\n",
    "\n",
    "**warning**\n",
    "- takes up considerable time to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca083446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download required NLTK data if not already present\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "    # Remove stopwords, lemmatize, and stem\n",
    "    words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words if word not in stop_words]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "for col in ['Title', 'Content']:\n",
    "    dataTrain[col] = dataTrain[col].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7687c",
   "metadata": {},
   "source": [
    "### Just printing out the firtst 5 columns to see what happend to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83ab4f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id                                  Title  \\\n",
      "0  262120               morgan fair condit crash   \n",
      "1  175132                 weigh electron guidanc   \n",
      "2  218739  fumbl testimoni x men director singer   \n",
      "3  253483             week recap eric hill drama   \n",
      "4  224109                                 letter   \n",
      "\n",
      "                                             Content  \n",
      "0  actor comedian morgan fair condit follow new j...  \n",
      "1  electron unexpectedli weak quarterli earn guid...  \n",
      "2  f said press confer around like meat sex direc...  \n",
      "3  mix happen tonight excit see want see happen e...  \n",
      "4  presid special tribut club music icon pen lett...  \n",
      "\n",
      "Data cleaning completed. Data shape: (47912, 3)\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 47912 entries, 0 to 47911\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   Id       47912 non-null  int64\n",
      " 1   Title    47912 non-null  str  \n",
      " 2   Content  47912 non-null  str  \n",
      "dtypes: int64(1), str(2)\n",
      "memory usage: 1.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dataTrain.head())\n",
    "print (\"\\nData cleaning completed. Data shape:\", dataTrain.shape)\n",
    "print (dataTrain.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a807e87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataTrain dataframe saved to joblibCache/dataTest_cleaned.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Save the dataTrain dataframe using joblib\n",
    "joblib.dump(dataTrain, 'joblibCache/dataTest_cleaned.joblib')\n",
    "print('dataTrain dataframe saved to joblibCache/dataTest_cleaned.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvEnv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
