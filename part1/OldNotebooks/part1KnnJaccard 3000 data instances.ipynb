{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50153835",
   "metadata": {},
   "source": [
    "## M161 first question notebook, Knn with jaccard distance classifier\n",
    "## Data preprocessing\n",
    "### Data cleaning I\n",
    " 1. check types \n",
    " 2. check for null values\n",
    " 3. check duplicates\n",
    " 4. keeping 10000 instances to reduce computation load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ccd11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "First 5 rows of the dataset:\n",
      "       Id                                              Title  \\\n",
      "0  227464  Netflix is coming to cable boxes, and Amazon i...   \n",
      "1  244074  Pharrell, Iranian President React to Tehran 'H...   \n",
      "2   60707                    Wildlife service seeks comments   \n",
      "3   27883  Facebook teams up with Storyful to launch 'FB ...   \n",
      "4  169596           Caesars plans US$880 mln New York casino   \n",
      "\n",
      "                                             Content          Label  \n",
      "0   if you subscribe to one of three rinky-dink (...  Entertainment  \n",
      "1   pharrell, iranian president react to tehran '...  Entertainment  \n",
      "2   the u.s. fish and wildlife service has reopen...     Technology  \n",
      "3   the very nature of social media means it is o...     Technology  \n",
      "4   caesars plans us$880 mln new york casino jul ...       Business  \n",
      "\n",
      "Data summary:\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 111795 entries, 0 to 111794\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count   Dtype\n",
      "---  ------   --------------   -----\n",
      " 0   Id       111795 non-null  int64\n",
      " 1   Title    111795 non-null  str  \n",
      " 2   Content  111795 non-null  str  \n",
      " 3   Label    111795 non-null  str  \n",
      "dtypes: int64(1), str(3)\n",
      "memory usage: 3.4 MB\n",
      "None\n",
      "\n",
      "Missing values in each column:\n",
      "Id         0\n",
      "Title      0\n",
      "Content    0\n",
      "Label      0\n",
      "dtype: int64\n",
      "\n",
      "Column data types:\n",
      "Id         int64\n",
      "Title        str\n",
      "Content      str\n",
      "Label        str\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = 'bigdata2025classification/train.csv'\n",
    "\n",
    "def load_and_process_data(file_path):\n",
    "    # Load data from a CSV file\n",
    "    dataTrain = pd.read_csv(file_path)\n",
    "\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(\"First 5 rows of the dataset:\")\n",
    "    print(dataTrain.head())\n",
    "\n",
    "    print(\"\\nData summary:\")\n",
    "    print(dataTrain.info())\n",
    "\n",
    "    # Check for missing values in the dataframe\n",
    "    print(\"\\nMissing values in each column:\")\n",
    "    print(dataTrain.isnull().sum())\n",
    "    \n",
    "    return dataTrain\n",
    "\n",
    "dataTrain = load_and_process_data(file_path)\n",
    "\n",
    "# check column data types\n",
    "def check_column_types(dataTrain):\n",
    "    print(\"\\nColumn data types:\")\n",
    "    print(dataTrain.dtypes)\n",
    "\n",
    "check_column_types(dataTrain)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cce109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset shape: (3000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Keep only the first 3000 instances for faster experimentation\n",
    "dataTrain = dataTrain.iloc[:3000].reset_index(drop=True)\n",
    "print(f\"Subset shape: {dataTrain.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b81952",
   "metadata": {},
   "source": [
    "## Data cleaning II continue\n",
    "3. check for duplicates\n",
    "***************************\n",
    "### note\n",
    "- the data types of all exeprt Id column is \"object\" in pandas, it works, but could be converted to String for a performance uplift.\n",
    "*****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf3640a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 0\n",
      "\n",
      "Number of duplicate rows based on Title: 5\n",
      "\n",
      "Number of duplicate rows based on Content: 6\n",
      "\n",
      "Number of duplicate rows based on Title and Content: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicate rows in the dataframe\n",
    "def check_duplicates(dataTrain):\n",
    "    duplicate_count = dataTrain.duplicated().sum()\n",
    "    print(f\"\\nNumber of duplicate rows: {duplicate_count}\")\n",
    "    return duplicate_count\n",
    "\n",
    "check_duplicates(dataTrain)\n",
    "\n",
    "# Check for duplicates based only on 'Title' column\n",
    "def check_title_duplicates(dataTrain):\n",
    "    if 'Title' in dataTrain.columns:\n",
    "        dup_count = dataTrain.duplicated(subset=['Title']).sum()\n",
    "        print(f\"\\nNumber of duplicate rows based on Title: {dup_count}\")\n",
    "        return dup_count\n",
    "    else:\n",
    "        print(\"'Title' column not found in the dataframe.\")\n",
    "        return None\n",
    "\n",
    "check_title_duplicates(dataTrain)\n",
    "\n",
    "# Check for duplicates based only on 'Content' column\n",
    "def check_content_duplicates(dataTrain):\n",
    "    if 'Content' in dataTrain.columns:\n",
    "        dup_count = dataTrain.duplicated(subset=['Content']).sum()\n",
    "        print(f\"\\nNumber of duplicate rows based on Content: {dup_count}\")\n",
    "        return dup_count\n",
    "    else:\n",
    "        print(\"'Content' column not found in the dataframe.\")\n",
    "        return None\n",
    "\n",
    "check_content_duplicates(dataTrain)\n",
    "# Check for duplicates based on 'Title' and 'Content' columns\n",
    "def check_title_content_duplicates(dataTrain):\n",
    "    if 'Title' in dataTrain.columns and 'Content' in dataTrain.columns:\n",
    "        dup_count = dataTrain.duplicated(subset=['Title', 'Content']).sum()\n",
    "        print(f\"\\nNumber of duplicate rows based on Title and Content: {dup_count}\")\n",
    "        return dup_count\n",
    "    else:\n",
    "        print(\"'Title' and/or 'Content' columns not found in the dataframe.\")\n",
    "        return None\n",
    "\n",
    "check_title_content_duplicates(dataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1deeda1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicates based on Title and Content removed. Data shape: (2997, 4)\n",
      "\n",
      "Index reset. Data shape: (2997, 4)\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 2997 entries, 0 to 2996\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   Id       2997 non-null   int64\n",
      " 1   Title    2997 non-null   str  \n",
      " 2   Content  2997 non-null   str  \n",
      " 3   Label    2997 non-null   str  \n",
      "dtypes: int64(1), str(3)\n",
      "memory usage: 93.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates based on 'Title' and 'Content' columns, keeping the first occurrence\n",
    "dataTrain = dataTrain.drop_duplicates(subset=['Title', 'Content'], keep='first')\n",
    "print(\"\\nDuplicates based on Title and Content removed. Data shape:\", dataTrain.shape)\n",
    "\n",
    "\n",
    "# Reset the index after removing duplicates\n",
    "dataTrain = dataTrain.reset_index(drop=True)\n",
    "print(\"\\nIndex reset. Data shape:\", dataTrain.shape)\n",
    "dataTrain.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a364cf5",
   "metadata": {},
   "source": [
    "\n",
    "### Remove words not in English dictionary\n",
    "\n",
    "- **probably could change dictionary for better results but it works...**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcc5b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Download the words corpus if not already present\n",
    "nltk.download('words')\n",
    "english_words = set(words.words())\n",
    "\n",
    "def clean_text(text):\n",
    "    # Split text into words\n",
    "    word_list = re.findall(r'\\b\\w+\\b', str(text))\n",
    "    cleaned_words = []\n",
    "    for word in word_list:\n",
    "        # Drop any word not in dictionary\n",
    "        if word.lower() not in english_words:\n",
    "            continue\n",
    "        # Drop words with 2+ repeating chars not in dictionary (redundant now, but kept for clarity)\n",
    "        if re.search(r'(.)\\1{1,}', word):\n",
    "            if word.lower() not in english_words:\n",
    "                continue\n",
    "        cleaned_words.append(word)\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Apply to both columns\n",
    "dataTrain['Title'] = dataTrain['Title'].apply(clean_text)\n",
    "dataTrain['Content'] = dataTrain['Content'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158fa07",
   "metadata": {},
   "source": [
    "## Text clean up \n",
    "1. Expand contractions\n",
    "2. Convert to lowercase\n",
    "3. Remove special characters (keep only letters and spaces)\n",
    "4. Remove extra spaces\n",
    "5. Remove stopwords, lemmatize, and stem\n",
    "\n",
    "**warning**\n",
    "- takes up considerable time to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4b0184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download required NLTK data if not already present\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "    # Remove stopwords, lemmatize, and stem\n",
    "    words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words if word not in stop_words]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "for col in ['Title', 'Content']:\n",
    "    dataTrain[col] = dataTrain[col].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee90bcf",
   "metadata": {},
   "source": [
    "### Just printing out the firtst 5 columns to see what happend to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68cb3ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id                       Title  \\\n",
      "0  227464  come cabl groceri overlord   \n",
      "1  244074          presid react happi   \n",
      "2   60707              wildlif servic   \n",
      "3   27883                      launch   \n",
      "4  169596           u new york casino   \n",
      "\n",
      "                                             Content          Label  \\\n",
      "0  subscrib one three dink compar speak cabl abl ...  Entertainment   \n",
      "1  presid react happi singer presid took twitter ...  Entertainment   \n",
      "2  fish wildlif servic comment period addit day p...     Technology   \n",
      "3  natur social medium often sourc real time brea...     Technology   \n",
      "4  u new york casino latest news top deck world e...       Business   \n",
      "\n",
      "   text_length  word_count  sentence_count  avg_word_length  \n",
      "0         1576         264              15         4.965909  \n",
      "1         1200         192              10         5.250000  \n",
      "2         2773         416              34         5.665865  \n",
      "3         1564         254              13         5.157480  \n",
      "4         2250         365              23         5.164384  \n"
     ]
    }
   ],
   "source": [
    "print(dataTrain.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d8862",
   "metadata": {},
   "source": [
    "## Starting future extraction (converting text to numbers for ML algorythms to run)\n",
    "- we should use **Bag of words** based on project requirements \n",
    "- Shoud take into account the title column in combination with content ()\n",
    "\n",
    "### ℹ️info \n",
    "\n",
    "- countvetrorizer does convert everyting to lowecase and removes punctuation by default. could remove steps from above Nltk powered code cell\n",
    "- Title and Content column are combined in to a sing string and then tokenized and vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca38533a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words matrix shape: (2997, 13088)\n",
      "dataTrain_bow sample (first 10 rows): [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Feature names (first 20): ['aa' 'aal' 'aam' 'aback' 'abacu' 'abandon' 'abat' 'abba' 'abbey'\n",
      " 'abdomen' 'abdomin' 'abduct' 'abe' 'aberr' 'abid' 'abigail' 'abil'\n",
      " 'abject' 'abl' 'ablaz']\n"
     ]
    }
   ],
   "source": [
    "# Combine 'Title' and 'Content' columns into a single string\n",
    "# and vectorize the result for classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a new column that combines Title and Content\n",
    "# (if either column is missing, fill with empty string)\n",
    "dataTrain['Combined'] = dataTrain['Title'].fillna('') + ' ' + dataTrain['Content'].fillna('')\n",
    "\n",
    "# Initialize CountVectorizer (Bag of Words)\n",
    "bow_vectorizer = CountVectorizer(binary=True, max_features=5000)  # Use binary=True for presence/absence of words, limit to top 5000 features\n",
    "\n",
    "# Fit and transform the combined column\n",
    "dataTrain_bow = bow_vectorizer.fit_transform(dataTrain['Combined'])\n",
    "\n",
    "# Show shape and a sample\n",
    "print('Bag of Words matrix shape:', dataTrain_bow.shape)\n",
    "print(\"dataTrain_bow sample (first 10 rows):\", dataTrain_bow[:10].toarray())\n",
    "print('Feature names (first 20):', bow_vectorizer.get_feature_names_out()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea5fdf",
   "metadata": {},
   "source": [
    "### visual check for weird words, repetitions, etc...\n",
    "- remember stemming and lemmatization was executed on text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e85f61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names (first 100): ['aa' 'aal' 'aam' 'aback' 'abacu' 'abandon' 'abat' 'abba' 'abbey'\n",
      " 'abdomen' 'abdomin' 'abduct' 'abe' 'aberr' 'abid' 'abigail' 'abil'\n",
      " 'abject' 'abl' 'ablaz' 'abli' 'abnorm' 'aboard' 'abolish' 'abolit'\n",
      " 'abomin' 'aborigin' 'abort' 'abound' 'abreast' 'abroad' 'abrupt'\n",
      " 'abruptli' 'absenc' 'absent' 'absolut' 'absolv' 'absorb' 'absorpt'\n",
      " 'abstain' 'abstin' 'abstract' 'abstrus' 'absurd' 'absurdli' 'abu' 'abund'\n",
      " 'abus' 'abut' 'abuzz' 'abysm' 'abyss' 'academ' 'academi' 'acceler'\n",
      " 'acceleromet' 'accent' 'accept' 'access' 'accessori' 'accid' 'accident'\n",
      " 'acclaim' 'accolad' 'accommod' 'accompani' 'accomplic' 'accomplish'\n",
      " 'accord' 'accordingli' 'accost' 'account' 'accredit' 'accret' 'accumul'\n",
      " 'accur' 'accuraci' 'accus' 'accustom' 'ace' 'acerb' 'acet' 'acetaldehyd'\n",
      " 'aceton' 'achiev' 'achingli' 'acid' 'acidif' 'ackman' 'acknowledg' 'acn'\n",
      " 'acoust' 'acquaint' 'acquir' 'acquisit' 'acquitt' 'acr' 'acreag' 'acrid'\n",
      " 'acrimoni' 'acrobat' 'acrolein' 'acronym' 'across' 'acryl' 'act' 'action'\n",
      " 'activ' 'activist' 'actor' 'actress' 'actual' 'actuari' 'acuiti' 'acumen'\n",
      " 'acut' 'ad' 'adad' 'adag' 'adam' 'adapt' 'add' 'addict' 'addit' 'address'\n",
      " 'ade' 'adenocarcinoma' 'adept' 'adequ' 'adher' 'adjac' 'adjud' 'adjunct'\n",
      " 'adjust' 'adjuv' 'admi' 'administ' 'administr' 'admir' 'admiralti'\n",
      " 'admiss' 'admit' 'admittedli' 'ado' 'adob' 'adolesc' 'adopt' 'ador'\n",
      " 'adoringli' 'adorn' 'adrenalin' 'adrift' 'adul' 'adult' 'adulteri'\n",
      " 'adulthood' 'advanc' 'advantag' 'adventur' 'advers' 'adversari' 'advert'\n",
      " 'advertis' 'advic' 'advis' 'advisor' 'advisori' 'advoc' 'advocaci' 'ae'\n",
      " 'aegi' 'aerial' 'aero' 'aerob' 'aerobat' 'aeronaut' 'aeroplan' 'aerosol'\n",
      " 'aesthet' 'afar' 'affabl' 'affair' 'affect' 'affection' 'affidavit'\n",
      " 'affili' 'affin' 'affirm' 'afflict' 'affluenc' 'affluent' 'afford'\n",
      " 'afield' 'afloat' 'afoot' 'afoul' 'afraid' 'afresh' 'aft' 'aftercar'\n",
      " 'afterhour' 'afterlif' 'aftermath' 'afternoon' 'aftershock'\n",
      " 'afterthought' 'afterward' 'aga' 'agap' 'age' 'ageless' 'agenc' 'agenda'\n",
      " 'agent' 'aggrav' 'aggreg' 'aggress' 'aggriev' 'agil' 'ago' 'agoni' 'agre'\n",
      " 'agreement' 'agricultur' 'agua' 'ah' 'aha' 'ahead' 'ahem' 'ahoy' 'ai'\n",
      " 'aid' 'ail' 'ailment' 'aim' 'air' 'airbrush' 'aircraft' 'airdrop' 'airi'\n",
      " 'airlin' 'airman' 'airplan' 'airport' 'airway' 'aisl' 'aka' 'ake' 'akin'\n",
      " 'al' 'ala' 'alamo' 'alan' 'alarm' 'alarmingli' 'alarmist' 'alba' 'albeit'\n",
      " 'albino' 'album' 'alchemi' 'alcohol' 'alderman' 'ale' 'alec' 'alert'\n",
      " 'alexia' 'alfa' 'alfalfa' 'alga' 'algorithm' 'ali' 'alia' 'alibi' 'alien'\n",
      " 'align' 'alik' 'aliment' 'alison' 'aliv' 'alkalin' 'allan' 'alleg'\n",
      " 'allegedli' 'allegi' 'allel' 'allerg' 'allergen' 'allergi' 'allevi'\n",
      " 'alley' 'alli' 'allianc' 'alloc' 'allopath' 'allow' 'alloy' 'allur'\n",
      " 'alma' 'almanac' 'almighti' 'almond' 'almost' 'alo' 'aloft' 'alon'\n",
      " 'along' 'alongsid' 'aloud' 'alpha' 'alphabet' 'alpin' 'alreadi' 'alright'\n",
      " 'also' 'alt' 'altar' 'alter' 'alterc' 'altern' 'althea' 'although'\n",
      " 'altimet' 'altitud' 'alto' 'altogeth' 'alum' 'alumina' 'aluminium'\n",
      " 'aluminum' 'alumnu' 'alway' 'amad' 'amalgam' 'amarillo' 'amass' 'amateur'\n",
      " 'amaz' 'amazingli' 'ambassador' 'amber' 'ambidextr' 'ambient' 'ambigu'\n",
      " 'ambit' 'ambiti' 'ambival' 'ambl' 'ambros' 'ambul' 'ame' 'amelia' 'amend'\n",
      " 'ami' 'amic' 'amid' 'amidst' 'amino' 'amir' 'amiss' 'ammonia' 'ammunit'\n",
      " 'amnesia' 'amnesti' 'amoeba' 'among' 'amongst' 'amoroso' 'amount'\n",
      " 'amphibi' 'amphitheat' 'ampl' 'ampli' 'amplif' 'amplifi' 'amput' 'amus'\n",
      " 'amygdala' 'amyloid' 'amyotroph' 'ana' 'anaconda' 'anaemia' 'anal'\n",
      " 'analog' 'analys' 'analysi' 'analyst' 'analyt' 'analyz' 'anaplasmosi'\n",
      " 'anarch' 'anarchi' 'anarchist' 'anathema' 'anatomi' 'ancestor' 'ancestri'\n",
      " 'anchor' 'anchorag' 'ancient' 'android' 'anecdot' 'anem' 'anemia'\n",
      " 'anesthesia' 'anesthesiologist' 'anesthet' 'aneurysm' 'anew' 'angel'\n",
      " 'anger' 'angl' 'angri' 'angrili' 'angst' 'anguish' 'ani' 'anil' 'anim'\n",
      " 'animatedli' 'animos' 'anisotrop' 'ankl' 'ann' 'anna' 'annal' 'annex'\n",
      " 'annihil' 'anniversari' 'announc' 'annoy' 'annoyingli' 'annual' 'annuiti'\n",
      " 'annul' 'anomal' 'anomali' 'anonym' 'anorexia' 'anoth' 'answer' 'ant'\n",
      " 'antagon' 'antagonist' 'antarct' 'antarctica' 'antelop' 'antenna'\n",
      " 'anthem' 'anthrax' 'anthropogen' 'anthropolog' 'anthropologist'\n",
      " 'anthropomorph' 'anti' 'antibacteri' 'antibiot' 'antibodi' 'antichrist'\n",
      " 'anticip' 'anticipatori' 'antidiabet' 'antidot' 'antifreez' 'antigen'\n",
      " 'antihero' 'antihistamin' 'antimalari' 'antioxid' 'antipodean' 'antiqu'\n",
      " 'antithesi' 'antitrust' 'antitrypsin' 'antivir' 'antiviru' 'anxieti'\n",
      " 'anxiou' 'anxious' 'anybodi' 'anyon' 'anyth' 'anyway' 'anywher' 'apa'\n",
      " 'apart' 'apartheid' 'apathet' 'apathi' 'ape' 'apertur' 'apex' 'apiec'\n",
      " 'aplast' 'apnea' 'apocalyps' 'apocalypt' 'apolog' 'apologet' 'apotheosi'\n",
      " 'appal' 'appallingli' 'appar' 'apparatu' 'apparel' 'appeal' 'appear'\n",
      " 'appeas' 'appel' 'appendix' 'appet' 'appetit' 'appl' 'applaud' 'applaus'\n",
      " 'appli' 'applianc' 'applic' 'appoint' 'appointe' 'apprais' 'appreci'\n",
      " 'apprehend' 'apprehens' 'apprentic' 'approach' 'appropri' 'approv'\n",
      " 'approvingli' 'approxim' 'apron' 'apropo' 'apt' 'aptitud' 'aptli' 'aqua'\n",
      " 'aquarium' 'aquat' 'ar' 'ara' 'arad' 'arango' 'arbit' 'arbitr'\n",
      " 'arbitrari' 'arbor' 'arboretum' 'arc' 'arcad' 'arcan' 'arch' 'archaic'\n",
      " 'archbishop' 'archer' 'archetyp' 'archipelago' 'architect' 'architectur'\n",
      " 'archiv' 'archivist' 'archway' 'arctic' 'ardent' 'arduou' 'are' 'area'\n",
      " 'arena' 'argentin' 'argu' 'argument' 'ari' 'ariel' 'aris' 'arisen'\n",
      " 'aristocraci' 'aristocrat' 'ark' 'arm' 'armament' 'armatur' 'armchair'\n",
      " 'armi' 'armig' 'armor' 'armori' 'armpit' 'armrest' 'aros' 'around'\n",
      " 'arous' 'arraign' 'arrang' 'array' 'arrest' 'arri' 'arriv' 'arrog'\n",
      " 'arrow' 'arsen' 'arson' 'art' 'artefact' 'arteri' 'arteriosclerot'\n",
      " 'arthriti' 'articl' 'articul' 'artifact' 'artific' 'artifici' 'artilleri'\n",
      " 'artisan' 'artist' 'artistri' 'as' 'ascend' 'ascens' 'ascent' 'ascertain'\n",
      " 'ascot' 'ash' 'asham' 'ashor' 'asid' 'ask' 'askew' 'asleep' 'asop' 'asp'\n",
      " 'aspart' 'aspect' 'aspen' 'aspir' 'aspirin' 'assassin' 'assault'\n",
      " 'assembl' 'assemblyman' 'assert' 'assess' 'asset' 'assign' 'assimil'\n",
      " 'assist' 'associ' 'assort' 'assuag' 'assum' 'assumpt' 'assur' 'asteroid'\n",
      " 'asthma' 'astonish' 'astonishingli' 'astound' 'astray' 'astrid' 'astring'\n",
      " 'astrolog' 'astronaut' 'astronom' 'astronomi' 'astrophys'\n",
      " 'astrophysicist' 'astut' 'asund' 'asylum' 'asymmetr' 'asymmetri'\n",
      " 'asymptomat' 'asynchron' 'ate' 'ateli' 'athlet' 'atla' 'atlant' 'atmo'\n",
      " 'atmospher' 'atom' 'aton' 'atop' 'atroci' 'attach' 'attack' 'attain'\n",
      " 'attempt' 'attend' 'attent' 'attest' 'attic' 'attir' 'attitud' 'attorney'\n",
      " 'attract' 'attribut' 'atyp' 'auburn' 'auction' 'audaci' 'audibl'\n",
      " 'audienc' 'audio' 'audiologist' 'audit' 'auditor' 'auditori' 'auditorium'\n",
      " 'augment' 'august' 'aunt' 'aura' 'aureu' 'aurora' 'auspex' 'auster'\n",
      " 'authent' 'author' 'authorit' 'authoritarian' 'autism' 'autist' 'auto'\n",
      " 'autobahn' 'autobiograph' 'autobiographi' 'autograph' 'autolog' 'automat'\n",
      " 'automobil' 'automot' 'autonom' 'autonomi' 'autopilot' 'autopsi' 'autumn'\n",
      " 'ava' 'avail' 'avalanch' 'avast' 'ave' 'aveng' 'avenu' 'averag' 'avers'\n",
      " 'avert' 'avian' 'aviat' 'avid' 'avocado' 'avoid' 'avow' 'avuncular' 'aw'\n",
      " 'await' 'awak' 'awaken' 'awar' 'award' 'awash' 'away' 'awe' 'awesom'\n",
      " 'awhil' 'awkward' 'awkwardli' 'axe' 'axi' 'axl' 'aye' 'azalea' 'azur'\n",
      " 'ba' 'babbl' 'babe' 'babi' 'bac' 'bachelor' 'back' 'backbon' 'backbreak'\n",
      " 'backdoor' 'backdrop' 'backer' 'backfir' 'background' 'backlash'\n",
      " 'backlog' 'backsid' 'backstag' 'backtrack' 'backup' 'backward' 'bacon'\n",
      " 'bacteri' 'bacteria' 'bacterium' 'bad' 'badg' 'badger' 'badli' 'bae'\n",
      " 'baffl' 'bafflingli' 'bag' 'bagel' 'baggag' 'bagger' 'baggi' 'bah' 'baht'\n",
      " 'bail' 'bailey' 'bain' 'bait' 'bake' 'baker' 'baku' 'balanc' 'balboa'\n",
      " 'balconi' 'bald' 'bale' 'bali' 'balk' 'balki' 'ball' 'ballad' 'ballast'\n",
      " 'ballet' 'ballist' 'balloon' 'ballot' 'ballroom' 'balsam' 'bam' 'bamboo'\n",
      " 'ban' 'banal' 'banana' 'banco' 'band' 'bandit' 'bang' 'banger' 'bangkok'\n",
      " 'banjo' 'bank' 'banker' 'bankrupt' 'bankruptci' 'banner' 'banter' 'bar'\n",
      " 'bara' 'barbar' 'barbarian' 'barbecu' 'barber' 'barbershop' 'barbican'\n",
      " 'barcelona' 'bard' 'bare' 'bareback' 'barefac' 'barefoot' 'barg'\n",
      " 'bargain' 'baria' 'bark' 'barker' 'barley' 'barn' 'barney' 'baro'\n",
      " 'baromet' 'baron' 'barra' 'barracuda' 'barrag' 'barrel' 'barren' 'barri'\n",
      " 'barrier' 'bartend' 'barth' 'barton' 'basal' 'base' 'basebal' 'baseless'\n",
      " 'baselessli' 'baseman' 'basement' 'bash' 'basi' 'basic' 'basin' 'bask'\n",
      " 'basket' 'basketbal' 'bass' 'bassist' 'basso' 'bastard' 'bat' 'batch'\n",
      " 'bateman' 'bath' 'bathroom' 'bathymetr' 'bathymetri' 'batman' 'baton'\n",
      " 'batt' 'battalion' 'batter' 'batteri' 'battl' 'battlefield'\n",
      " 'battleground' 'bauxit' 'baxter' 'bay' 'bayonet' 'bayou' 'bazaar'\n",
      " 'bazooka' 'beach' 'beachi' 'beacon' 'bead' 'beak' 'beam' 'bean' 'beani'\n",
      " 'bear' 'bearabl' 'beard' 'bearer' 'bearish' 'beast' 'beasti' 'beat'\n",
      " 'beaten' 'beau' 'beauti' 'beaver' 'becker' 'becom' 'bed' 'bedrock'\n",
      " 'bedroom' 'bedsid' 'bedsor' 'bedtim' 'bee' 'beech' 'beef' 'beek' 'beelin'\n",
      " 'beer' 'beetl' 'befal' 'befit' 'beforehand' 'beg' 'beggar' 'begin'\n",
      " 'begun' 'behalf' 'behav' 'behavior' 'behaviour' 'behemoth' 'behest'\n",
      " 'behind' 'behold' 'beholden' 'behoov' 'beig' 'bel' 'belatedli' 'belief'\n",
      " 'believ' 'bell' 'belli' 'belliger' 'bellow' 'belong' 'belov' 'belt'\n",
      " 'belveder' 'bemoan' 'bemus' 'ben' 'bench' 'bend' 'beneath']\n"
     ]
    }
   ],
   "source": [
    "print('Feature names (first 100):', bow_vectorizer.get_feature_names_out()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d728f40",
   "metadata": {},
   "source": [
    "## KNN with jaccard distance classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9731f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github\\bigData\\venvEnv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2459: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "d:\\Github\\bigData\\venvEnv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2459: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "d:\\Github\\bigData\\venvEnv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2459: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "d:\\Github\\bigData\\venvEnv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2459: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "d:\\Github\\bigData\\venvEnv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2459: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (Jaccard) 5-fold CV accuracy scores: [0.86833333 0.86666667 0.86978297 0.86477462 0.86477462]\n",
      "Mean CV accuracy: 0.8668664440734558\n",
      "KNN (Jaccard) 5-fold CV Jaccard scores: [0.86833333 0.86666667 0.86978297 0.86477462 0.86477462]\n",
      "Mean CV Jaccard score: 0.8668664440734558\n"
     ]
    }
   ],
   "source": [
    "# KNN classification with Jaccard distance and 5-fold cross-validation on Bag of Words features\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import jaccard_score, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Use the same Bag of Words features as before\n",
    "X = dataTrain_bow.toarray()\n",
    "y = dataTrain['Label'].values\n",
    "\n",
    "# Initialize KNN classifier with Jaccard distance (metric='jaccard')\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5, metric='jaccard', n_jobs=-1)\n",
    "\n",
    "# Perform 5-fold cross-validation using accuracy\n",
    "cv_scores_acc = cross_val_score(knn_clf, X, y, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "print('KNN (Jaccard) 5-fold CV accuracy scores:', cv_scores_acc)\n",
    "print('Mean CV accuracy:', np.mean(cv_scores_acc))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvEnv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
