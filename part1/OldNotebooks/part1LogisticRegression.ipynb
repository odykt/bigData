{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50153835",
   "metadata": {},
   "source": [
    "\n",
    "## M161 first question notebook\n",
    "    - created venv python environment\n",
    "    - shloud create requirements for resusability\n",
    "    - data .csv files to large to upload to github will resolve thaht later\n",
    "## Data cleaning I\n",
    " 1.  check types \n",
    " 2. check for null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fd9aacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "First 5 rows of the dataset:\n",
      "       Id                                              Title  \\\n",
      "0  227464  Netflix is coming to cable boxes, and Amazon i...   \n",
      "1  244074  Pharrell, Iranian President React to Tehran 'H...   \n",
      "2   60707                    Wildlife service seeks comments   \n",
      "3   27883  Facebook teams up with Storyful to launch 'FB ...   \n",
      "4  169596           Caesars plans US$880 mln New York casino   \n",
      "\n",
      "                                             Content          Label  \n",
      "0   if you subscribe to one of three rinky-dink (...  Entertainment  \n",
      "1   pharrell, iranian president react to tehran '...  Entertainment  \n",
      "2   the u.s. fish and wildlife service has reopen...     Technology  \n",
      "3   the very nature of social media means it is o...     Technology  \n",
      "4   caesars plans us$880 mln new york casino jul ...       Business  \n",
      "\n",
      "Data summary:\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 111795 entries, 0 to 111794\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count   Dtype\n",
      "---  ------   --------------   -----\n",
      " 0   Id       111795 non-null  int64\n",
      " 1   Title    111795 non-null  str  \n",
      " 2   Content  111795 non-null  str  \n",
      " 3   Label    111795 non-null  str  \n",
      "dtypes: int64(1), str(3)\n",
      "memory usage: 3.4 MB\n",
      "None\n",
      "\n",
      "Missing values in each column:\n",
      "Id         0\n",
      "Title      0\n",
      "Content    0\n",
      "Label      0\n",
      "dtype: int64\n",
      "\n",
      "Column data types:\n",
      "Id         int64\n",
      "Title        str\n",
      "Content      str\n",
      "Label        str\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = 'bigdata2025classification/train.csv'\n",
    "\n",
    "def load_and_process_data(file_path):\n",
    "    # Load data from a CSV file\n",
    "    dataTrain = pd.read_csv(file_path)\n",
    "\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(\"First 5 rows of the dataset:\")\n",
    "    print(dataTrain.head())\n",
    "\n",
    "    print(\"\\nData summary:\")\n",
    "    print(dataTrain.info())\n",
    "\n",
    "    # Check for missing values in the dataframe\n",
    "    print(\"\\nMissing values in each column:\")\n",
    "    print(dataTrain.isnull().sum())\n",
    "    \n",
    "    return dataTrain\n",
    "\n",
    "dataTrain = load_and_process_data(file_path)\n",
    "\n",
    "# check column data types\n",
    "def check_column_types(dataTrain):\n",
    "    print(\"\\nColumn data types:\")\n",
    "    print(dataTrain.dtypes)\n",
    "\n",
    "check_column_types(dataTrain)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3cce109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset shape: (6000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Keep only the first 6000 instances for faster experimentation\n",
    "dataTrain = dataTrain.iloc[:6000].reset_index(drop=True)\n",
    "print(f\"Subset shape: {dataTrain.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b81952",
   "metadata": {},
   "source": [
    "## Data cleaning II continue\n",
    "3. check for duplicates\n",
    "***************************\n",
    "### note\n",
    "- the data types of all exeprt Id column is \"object\" in pandas, it works, but could be converted to String for a performance uplift.\n",
    "*****************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39741b3a",
   "metadata": {},
   "source": [
    "## Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1deeda1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicates based on Title and Content removed. Data shape: (5996, 4)\n",
      "\n",
      "Index reset. Data shape: (5996, 4)\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 5996 entries, 0 to 5995\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   Id       5996 non-null   int64\n",
      " 1   Title    5996 non-null   str  \n",
      " 2   Content  5996 non-null   str  \n",
      " 3   Label    5996 non-null   str  \n",
      "dtypes: int64(1), str(3)\n",
      "memory usage: 187.5 KB\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates based on 'Title' and 'Content' columns, keeping the first occurrence\n",
    "dataTrain = dataTrain.drop_duplicates(subset=['Title', 'Content'], keep='first')\n",
    "print(\"\\nDuplicates based on Title and Content removed. Data shape:\", dataTrain.shape)\n",
    "\n",
    "\n",
    "# Reset the index after removing duplicates\n",
    "dataTrain = dataTrain.reset_index(drop=True)\n",
    "print(\"\\nIndex reset. Data shape:\", dataTrain.shape)\n",
    "dataTrain.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f3320",
   "metadata": {},
   "source": [
    "\n",
    "### **fixed**\n",
    "*******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f964a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Content Statistics ---\n",
      "        text_length   word_count  sentence_count  avg_word_length\n",
      "count   5996.000000  5996.000000     5996.000000      5996.000000\n",
      "mean    2560.841561   422.743829       24.667612         5.065045\n",
      "std     2107.402479   350.217259       24.484260         0.363527\n",
      "min       17.000000     3.000000        1.000000         3.953704\n",
      "25%     1293.750000   214.000000       12.000000         4.834019\n",
      "50%     2023.000000   336.000000       19.000000         5.055181\n",
      "75%     3154.000000   517.000000       30.000000         5.269693\n",
      "max    43621.000000  7044.000000      846.000000        12.540541\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dataTrain['text_length'] = dataTrain['Content'].apply(len)\n",
    "dataTrain['word_count'] = dataTrain['Content'].apply(lambda x: len(str(x).split()))\n",
    "dataTrain['sentence_count'] = dataTrain['Content'].apply(lambda x: len(str(x).split('.')))\n",
    "dataTrain['avg_word_length'] = dataTrain['Content'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))\n",
    "\n",
    "print(\"\\n--- Content Statistics ---\")\n",
    "print(dataTrain[['text_length', 'word_count', 'sentence_count', 'avg_word_length']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a364cf5",
   "metadata": {},
   "source": [
    "\n",
    "### Remove words not in English dictionary\n",
    "\n",
    "- **probably could change dictionary for better results but it works...**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcc5b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Download the words corpus if not already present\n",
    "nltk.download('words')\n",
    "english_words = set(words.words())\n",
    "\n",
    "def clean_text(text):\n",
    "    # Split text into words\n",
    "    word_list = re.findall(r'\\b\\w+\\b', str(text))\n",
    "    cleaned_words = []\n",
    "    for word in word_list:\n",
    "        # Drop any word not in dictionary\n",
    "        if word.lower() not in english_words:\n",
    "            continue\n",
    "        # Drop words with 2+ repeating chars not in dictionary (redundant now, but kept for clarity)\n",
    "        if re.search(r'(.)\\1{1,}', word):\n",
    "            if word.lower() not in english_words:\n",
    "                continue\n",
    "        cleaned_words.append(word)\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Apply to both columns\n",
    "dataTrain['Title'] = dataTrain['Title'].apply(clean_text)\n",
    "dataTrain['Content'] = dataTrain['Content'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec666e",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "- TAKES A LOT OF TIME TO EXECUTE ~7 min\n",
    "*********************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac4b0184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download required NLTK data if not already present\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "    # Remove stopwords, lemmatize, and stem\n",
    "    words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words if word not in stop_words]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "for col in ['Title', 'Content']:\n",
    "    dataTrain[col] = dataTrain[col].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68cb3ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id                       Title  \\\n",
      "0  227464  come cabl groceri overlord   \n",
      "1  244074          presid react happi   \n",
      "2   60707              wildlif servic   \n",
      "3   27883                      launch   \n",
      "4  169596           u new york casino   \n",
      "\n",
      "                                             Content          Label  \\\n",
      "0  subscrib one three dink compar speak cabl abl ...  Entertainment   \n",
      "1  presid react happi singer presid took twitter ...  Entertainment   \n",
      "2  fish wildlif servic comment period addit day p...     Technology   \n",
      "3  natur social medium often sourc real time brea...     Technology   \n",
      "4  u new york casino latest news top deck world e...       Business   \n",
      "\n",
      "   text_length  word_count  sentence_count  avg_word_length  \n",
      "0         1576         264              15         4.965909  \n",
      "1         1200         192              10         5.250000  \n",
      "2         2773         416              34         5.665865  \n",
      "3         1564         254              13         5.157480  \n",
      "4         2250         365              23         5.164384  \n"
     ]
    }
   ],
   "source": [
    "print(dataTrain.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d8862",
   "metadata": {},
   "source": [
    "## Starting future extraction (converting text to numbers for ML algorythms to run)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca38533a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words matrix shape: (5996, 5000)\n",
      "dataTrain_bow sample (first 10 rows): [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Feature names (first 20): ['abandon' 'abdomin' 'abil' 'abl' 'abl get' 'abl see' 'abl use' 'aboard'\n",
      " 'abroad' 'absenc' 'absent' 'absolut' 'absorb' 'absurd' 'abund' 'abus'\n",
      " 'academ' 'academi' 'academi award' 'acceler']\n"
     ]
    }
   ],
   "source": [
    "# Combine 'Title' and 'Content' columns into a single string\n",
    "# and vectorize the result for classification\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a new column that combines Title and Content\n",
    "# (if either column is missing, fill with empty string)\n",
    "dataTrain['Combined'] = dataTrain['Title'].fillna('') + ' ' + dataTrain['Content'].fillna('')\n",
    "\n",
    "# Initialize CountVectorizer (Bag of Words)\n",
    "bow_vectorizer = CountVectorizer(binary=True, max_features=5000, ngram_range=(1,2))  # Use binary=True for presence/absence of words, limit to top 5000 features no limit shape (111220,27139)\n",
    "\n",
    "# Fit and transform the combined column\n",
    "dataTrain_bow = bow_vectorizer.fit_transform(dataTrain['Combined'])\n",
    "\n",
    "# Show shape and a sample\n",
    "print('Bag of Words matrix shape:', dataTrain_bow.shape)\n",
    "print(\"dataTrain_bow sample (first 10 rows):\", dataTrain_bow[:10].toarray())\n",
    "print('Feature names (first 20):', bow_vectorizer.get_feature_names_out()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e85f61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names (first 100): ['abandon' 'abdomin' 'abil' 'abl' 'abl get' 'abl see' 'abl use' 'aboard'\n",
      " 'abroad' 'absenc' 'absent' 'absolut' 'absorb' 'absurd' 'abund' 'abus'\n",
      " 'academ' 'academi' 'academi award' 'acceler' 'accept' 'access' 'accid'\n",
      " 'accident' 'accommod' 'accomplish' 'accord' 'accord compani'\n",
      " 'accord data' 'accord diseas' 'accord nation' 'accord new' 'accord news'\n",
      " 'accord recent' 'accord report' 'accord statement' 'accord studi'\n",
      " 'accord world' 'accordingli' 'account' 'accur' 'accuraci' 'accus'\n",
      " 'achiev' 'acknowledg' 'acquir' 'acquisit' 'across' 'across countri'\n",
      " 'across world' 'act' 'action' 'activ' 'activist' 'actor' 'actress'\n",
      " 'actual' 'acut' 'ad' 'ad new' 'adapt' 'add' 'addict' 'addit' 'address'\n",
      " 'adequ' 'adjust' 'administr' 'administr said' 'admir' 'admiss' 'admit'\n",
      " 'adopt' 'ador' 'adult' 'advanc' 'advantag' 'adventur' 'advers' 'advertis'\n",
      " 'advertis continu' 'advic' 'advis' 'advisori' 'advocaci' 'aesthet'\n",
      " 'affair' 'affect' 'affili' 'afford' 'afford care' 'afraid' 'aftermath'\n",
      " 'afternoon' 'afterward' 'age' 'age extinct' 'agenc' 'agenc said' 'agenda'\n",
      " 'agent' 'aggress' 'ago' 'agre' 'agre pay' 'agreement' 'agricultur'\n",
      " 'ahead' 'aid' 'aim' 'air' 'air forc' 'aircraft' 'airplan' 'airport'\n",
      " 'aisl' 'aka' 'al' 'alan' 'alarm' 'albeit' 'album' 'alcohol' 'alert'\n",
      " 'alien' 'align' 'alik' 'aliv' 'allegedli' 'alli' 'allianc' 'allow'\n",
      " 'almost' 'almost million' 'alon' 'along' 'along way' 'alongsid' 'alpha'\n",
      " 'alreadi' 'also' 'also abl' 'also avail' 'also come' 'also featur'\n",
      " 'also found' 'also get' 'also includ' 'also known' 'also like'\n",
      " 'also made' 'also make' 'also new' 'also note' 'also one' 'also provid'\n",
      " 'also receiv' 'also recent' 'also reveal' 'also said' 'also see'\n",
      " 'also seen' 'also star' 'also take' 'also took' 'also use' 'also work'\n",
      " 'also would' 'alter' 'altern' 'although' 'altitud' 'altogeth' 'aluminum'\n",
      " 'alway' 'amaz' 'amaz spider' 'ambassador' 'amber' 'ambiti' 'amend' 'ami'\n",
      " 'amid' 'among' 'amongst' 'amount' 'amount time' 'amus' 'analysi'\n",
      " 'analyst' 'analyst said' 'analyt' 'anchor' 'ancient' 'android'\n",
      " 'android oper' 'android phone' 'android wear' 'angel' 'anger' 'angl'\n",
      " 'angri' 'anim' 'ann' 'anna' 'anniversari' 'announc' 'annoy' 'annual'\n",
      " 'anonym' 'anoth' 'anoth one' 'answer' 'anti' 'anticip' 'antitrust'\n",
      " 'anxieti' 'anybodi' 'anyon' 'anyon els' 'anyth' 'anyth els' 'anyway'\n",
      " 'anywher' 'apart' 'apolog' 'appar' 'apparel' 'appeal' 'appear' 'appetit'\n",
      " 'appl' 'appl also' 'appl appl' 'appl could' 'appl may' 'appl new'\n",
      " 'appl said' 'appl store' 'appl would' 'appli' 'applic' 'appoint'\n",
      " 'appreci' 'approach' 'appropri' 'approv' 'approxim' 'arc' 'architectur'\n",
      " 'archiv' 'arctic' 'area' 'arena' 'argu' 'argument' 'arm' 'armi' 'armor'\n",
      " 'around' 'around countri' 'around million' 'around per' 'around percent'\n",
      " 'around time' 'around world' 'arrang' 'array' 'arrest' 'arriv' 'art'\n",
      " 'articl' 'articl inform' 'articl may' 'artifici' 'artifici intellig'\n",
      " 'artist' 'as' 'asid' 'ask' 'aspect' 'aspir' 'assassin' 'assault'\n",
      " 'assembl' 'assess' 'asset' 'assign' 'assist' 'assist professor' 'associ'\n",
      " 'associ press' 'associ professor' 'assum' 'assur' 'astronaut' 'astronom'\n",
      " 'ate' 'athlet' 'atlant' 'atmospher' 'attach' 'attack' 'attempt' 'attend'\n",
      " 'attent' 'attitud' 'attorney' 'attorney gener' 'attract' 'auction'\n",
      " 'audienc' 'audio' 'august' 'authent' 'author' 'auto' 'automat'\n",
      " 'automobil' 'automot' 'autonom' 'autumn' 'avail' 'avenu' 'averag'\n",
      " 'averag price' 'aviat' 'avoid' 'aw' 'awar' 'award' 'award win' 'away'\n",
      " 'awesom' 'awkward' 'azalea' 'babi' 'bachelor' 'back' 'back forth'\n",
      " 'back new' 'back time' 'back togeth' 'backdrop' 'background' 'backlash'\n",
      " 'backstag' 'backup' 'bacteria' 'bad' 'bad news' 'badli' 'bag' 'bail'\n",
      " 'bake' 'baker' 'balanc' 'balanc sheet' 'ball' 'ban' 'band' 'bang'\n",
      " 'bang theori' 'bank' 'bank said' 'bankruptci' 'banner' 'bar' 'bare'\n",
      " 'barra' 'barrel' 'barri' 'barrier' 'base' 'base compani' 'basebal' 'basi'\n",
      " 'basic' 'basketbal' 'bat' 'batch' 'bathroom' 'batman' 'batteri'\n",
      " 'batteri life' 'battl' 'bay' 'bay area' 'beach' 'beam' 'bean' 'bear'\n",
      " 'beard' 'beast' 'beat' 'beauti' 'becom' 'becom one' 'bed' 'beef' 'beer'\n",
      " 'begin' 'begun' 'behalf' 'behavior' 'behaviour' 'behind' 'belief'\n",
      " 'believ' 'bell' 'belong' 'belov' 'belt' 'ben' 'beneath' 'benefit'\n",
      " 'benjamin' 'berlin' 'besid' 'best' 'best friend' 'best known' 'best sell'\n",
      " 'best way' 'bet' 'beta' 'better' 'beverag' 'bey' 'beyond' 'bid' 'big'\n",
      " 'big bang' 'big day' 'big deal' 'big screen' 'bigger' 'biggest' 'bikini'\n",
      " 'bill' 'billboard' 'billi' 'billion' 'billion billion' 'billion cash'\n",
      " 'billion deal' 'billion last' 'billion peopl' 'billion per'\n",
      " 'billion year' 'billionair' 'bing' 'biolog' 'bird' 'birth' 'birthday'\n",
      " 'bit' 'bite' 'bitter' 'bizarr' 'black' 'black white' 'blackberri' 'blake'\n",
      " 'blame' 'blast' 'bleed' 'blend' 'bless' 'blind' 'block' 'blockbust'\n",
      " 'blond' 'blood' 'blood pressur' 'bloodi' 'bloom' 'blow' 'blown' 'blue'\n",
      " 'blunt' 'blur' 'bo' 'board' 'boast' 'boat' 'bob' 'bodi' 'bold' 'bolster'\n",
      " 'bomb' 'bon' 'bond' 'bone' 'bonu' 'book' 'boom' 'boost' 'booster' 'boot'\n",
      " 'border' 'bore' 'born' 'borrow' 'boston' 'bottl' 'bottom' 'bottom line'\n",
      " 'bought' 'bounc' 'bound' 'bow' 'bowl' 'box' 'box offic' 'boy' 'brad'\n",
      " 'brain' 'branch' 'brand' 'brand new' 'brave' 'brazil' 'breach' 'break'\n",
      " 'break bad' 'break news' 'breakdown' 'breakfast' 'breakthrough' 'breakup'\n",
      " 'breast' 'breast cancer' 'breath' 'breed' 'brent' 'bride' 'bridg' 'brief'\n",
      " 'briefli' 'bright' 'brilliant' 'bring' 'bring back' 'broad' 'broadcast'\n",
      " 'broadli' 'broadway' 'broke' 'broken' 'broker' 'brother' 'brought'\n",
      " 'brown' 'brows' 'browser' 'browser video' 'brunett' 'brush' 'brutal' 'bu'\n",
      " 'bubbl' 'buddi' 'budget' 'bug' 'build' 'built' 'bulk' 'bull' 'bullet'\n",
      " 'bullish' 'bump' 'bunch' 'bundl' 'burden' 'bureau' 'buri' 'burn' 'burst'\n",
      " 'bush' 'busi' 'busi day' 'busi model' 'busi said' 'bust' 'button' 'buy'\n",
      " 'buyer' 'buzz' 'ca' 'cabl' 'cake' 'calcul' 'calendar' 'call' 'calm'\n",
      " 'came' 'came back' 'cameo' 'camera' 'camp' 'campaign' 'campu' 'canada'\n",
      " 'cancel' 'cancer' 'cancer societi' 'candi' 'candid' 'cannot' 'cap'\n",
      " 'capabl' 'capac' 'cape' 'capit' 'captain' 'captain winter' 'caption'\n",
      " 'captur' 'car' 'carbon' 'carbon dioxid' 'card' 'cardiovascular'\n",
      " 'cardiovascular diseas' 'care' 'care act' 'career' 'cargo' 'carl'\n",
      " 'carney' 'carpet' 'carri' 'carrier' 'carter' 'cartoon' 'case' 'cash'\n",
      " 'cash flow' 'cast' 'castl' 'casual' 'cat' 'catastroph' 'catch' 'categori'\n",
      " 'caught' 'caus' 'caution' 'cautiou' 'ceas' 'celebr' 'cell' 'cent'\n",
      " 'center' 'center new' 'central' 'central bank' 'centuri' 'centuri fox'\n",
      " 'ceremoni' 'certain' 'certainli' 'certif' 'certifi' 'chain' 'chair'\n",
      " 'chairman' 'chairman said' 'challeng' 'champion' 'chanc' 'chang'\n",
      " 'channel' 'chao' 'chapter' 'charact' 'charg' 'chariti' 'charm' 'chart'\n",
      " 'charter' 'chase' 'chat' 'cheap' 'cheat' 'check' 'check video' 'cheer'\n",
      " 'chef' 'chemic' 'chemistri' 'chest' 'chicken' 'chief' 'chief economist'\n",
      " 'chief execut' 'chief financi' 'chief market' 'chief oper' 'child'\n",
      " 'childhood' 'china' 'china said' 'chip' 'chocol' 'choic' 'choos' 'choru'\n",
      " 'chose' 'chosen' 'chrome' 'chronic' 'chronicl' 'church' 'cigarett'\n",
      " 'cinema' 'cinemat' 'circl' 'circuit' 'citi' 'citizen' 'civil' 'clad'\n",
      " 'claim' 'clan' 'clariti' 'clark' 'class' 'classic' 'classifi' 'clean'\n",
      " 'clear' 'clearli' 'clever' 'click' 'client' 'climat' 'climat chang'\n",
      " 'climb' 'clinic' 'clip' 'clock' 'close' 'close comment' 'close contact'\n",
      " 'closer' 'closer look' 'closur' 'cloth' 'cloud' 'club' 'coach' 'coal'\n",
      " 'coalit' 'coast' 'coastal' 'coat' 'cobalt' 'cocktail' 'code' 'coffe'\n",
      " 'cognit' 'coincid' 'cold' 'cole' 'coli' 'colin' 'collabor' 'collaps'\n",
      " 'colleagu' 'collect' 'colleg' 'color' 'colorado' 'colour' 'column'\n",
      " 'columnist' 'combat' 'combin' 'combin compani' 'come' 'come across'\n",
      " 'come back' 'come close' 'come day' 'come end' 'come new' 'come one'\n",
      " 'come said' 'come soon' 'come togeth' 'come true' 'come two' 'comeback'\n",
      " 'comed' 'comedi' 'comedian' 'comet' 'comfort' 'comic' 'comic book'\n",
      " 'comic con' 'command' 'comment' 'comment get' 'commentari' 'commerc'\n",
      " 'commerc depart' 'commerci' 'commiss' 'commiss said' 'commission'\n",
      " 'commit' 'committe' 'commod' 'common' 'commonli' 'commun' 'compact'\n",
      " 'compani' 'compani also' 'compani billion' 'compani million'\n",
      " 'compani new' 'compani said' 'compani work' 'compani would' 'companion'\n",
      " 'compar' 'comparison' 'compass' 'compat' 'compel' 'compens' 'compet'\n",
      " 'competit' 'competitor' 'complain' 'complaint' 'complement' 'complet'\n",
      " 'complex' 'compli' 'complianc' 'complic' 'compon' 'compos' 'composit'\n",
      " 'composit index' 'comprehens' 'compris' 'compromis' 'comput' 'con'\n",
      " 'conceal' 'conceiv' 'concentr' 'concept' 'concern' 'concert' 'conclud'\n",
      " 'conclus' 'concret' 'condemn' 'condit' 'conduct' 'confer' 'confer call'\n",
      " 'confid' 'confidenti' 'confin' 'confirm' 'conflict' 'confront' 'confus'\n",
      " 'congress' 'congression' 'connect' 'consciou' 'conscious' 'consecut'\n",
      " 'consensu' 'consent' 'consequ' 'conserv' 'consid' 'consid web' 'consider'\n",
      " 'consist' 'consol' 'consolid' 'conspiraci' 'constant' 'constantli'\n",
      " 'constitut' 'construct' 'consult' 'consum' 'consum spend' 'consumpt'\n",
      " 'contact' 'contagi' 'contain' 'contamin' 'contemporari' 'content'\n",
      " 'contest' 'context' 'contin' 'continu' 'continu read' 'contract'\n",
      " 'contrari' 'contrast' 'contribut' 'contributor' 'control'\n",
      " 'control prevent' 'controversi' 'conveni' 'convent' 'convers'\n",
      " 'convers tweet' 'convert' 'convict' 'convinc' 'cook' 'cool' 'cooper'\n",
      " 'cope' 'copi' 'copper' 'copyright' 'copyright associ' 'copyright reserv'\n",
      " 'core' 'core processor' 'core snapdragon' 'corn' 'corner' 'corp' 'corpor'\n",
      " 'correct' 'correspond' 'corrupt' 'cosmic' 'cost' 'cost million' 'costli'\n",
      " 'costum' 'couch' 'cough' 'could' 'could also' 'could becom' 'could come'\n",
      " 'could easili' 'could end' 'could get' 'could go' 'could help'\n",
      " 'could lead' 'could make' 'could possibl' 'could see' 'could still'\n",
      " 'could take' 'could use' 'could well' 'council' 'counsel' 'count'\n",
      " 'counter' 'counterpart' 'counti' 'countless' 'countri' 'coupl' 'courag'\n",
      " 'cours' 'court' 'court decis' 'court rule' 'courtesi' 'cousin' 'cover'\n",
      " 'coverag' 'crack' 'craft' 'crash' 'crazi' 'cream' 'creat' 'creat new'\n",
      " 'creation' 'creativ' 'creator' 'creatur' 'credibl']\n"
     ]
    }
   ],
   "source": [
    "print('Feature names (first 100):', bow_vectorizer.get_feature_names_out()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e45df4",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier\n",
    "\n",
    "Now we will train a Logistic Regression classifier using the Bag of Words features extracted above. We will evaluate its performance using accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa7831c",
   "metadata": {},
   "source": [
    "### 5-Fold Cross-Validation\n",
    "\n",
    "We will now use 5-fold cross-validation to evaluate the Logistic Regression classifier, instead of a single train/test split. This provides a more robust estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d7c3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# # Assume the target column is named 'Label' (change if needed)\n",
    "# if 'Label' not in dataTrain.columns:\n",
    "#     print(\"ERROR: 'Label' column not found in dataTrain. Please check your dataset.\")\n",
    "# else:\n",
    "#     # Split data into features and target\n",
    "#     X = dataTrain_bow\n",
    "#     y = dataTrain['Label']\n",
    "\n",
    "#     # Split into train and test sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     # Initialize and train Logistic Regression\n",
    "#     clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "#     # Predict on test set\n",
    "#     y_pred = clf.predict(X_test)\n",
    "\n",
    "#     # Evaluate\n",
    "#     print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#     print(\"Precision:\", precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "#     print(\"Recall:\", recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "#     print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "#     print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6df93c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation Accuracy Scores: [0.89416667 0.88907423 0.89908257 0.88573812 0.89574646]\n",
      "Mean Accuracy: 0.8927616068946345\n",
      "Std Dev: 0.0047699951622402585\n",
      "\n",
      "Classification Report (5-fold CV):\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     Business       0.84      0.84      0.84      1335\n",
      "Entertainment       0.94      0.95      0.94      2407\n",
      "       Health       0.92      0.89      0.91       636\n",
      "   Technology       0.86      0.85      0.85      1618\n",
      "\n",
      "     accuracy                           0.89      5996\n",
      "    macro avg       0.89      0.88      0.89      5996\n",
      " weighted avg       0.89      0.89      0.89      5996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assume the target column is named 'Label' (change if needed)\n",
    "if 'Label' not in dataTrain.columns:\n",
    "    print(\"ERROR: 'Label' column not found in dataTrain. Please check your dataset.\")\n",
    "else:\n",
    "    X = dataTrain_bow\n",
    "    y = dataTrain['Label']\n",
    "    clf = LogisticRegression(max_iter=3000,class_weight='balanced',random_state=42)\n",
    "\n",
    "    # 5-fold cross-validation scores\n",
    "    accuracy_scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"5-Fold Cross-Validation Accuracy Scores:\", accuracy_scores)\n",
    "    print(\"Mean Accuracy:\", accuracy_scores.mean())\n",
    "    print(\"Std Dev:\", accuracy_scores.std())\n",
    "\n",
    "    # Cross-validated predictions for full classification report\n",
    "    y_pred_cv = cross_val_predict(clf, X, y, cv=5)\n",
    "    print(\"\\nClassification Report (5-fold CV):\\n\", classification_report(y, y_pred_cv, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvEnv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
