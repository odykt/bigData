{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50153835",
   "metadata": {},
   "source": [
    "# M161 first question notebook- best model LogisticRegrgessionCV , 15000 instances and max_features=10000 vectorization\n",
    "\n",
    "## Data preprocessing\n",
    "### Data cleaning I\n",
    " 1. check types \n",
    " 2. check for null values\n",
    " 3. check duplicates\n",
    " 4. keeping 10000 instances to reduce computation load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd9aacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "First 5 rows of the dataset:\n",
      "       Id                                              Title  \\\n",
      "0  227464  Netflix is coming to cable boxes, and Amazon i...   \n",
      "1  244074  Pharrell, Iranian President React to Tehran 'H...   \n",
      "2   60707                    Wildlife service seeks comments   \n",
      "3   27883  Facebook teams up with Storyful to launch 'FB ...   \n",
      "4  169596           Caesars plans US$880 mln New York casino   \n",
      "\n",
      "                                             Content          Label  \n",
      "0   if you subscribe to one of three rinky-dink (...  Entertainment  \n",
      "1   pharrell, iranian president react to tehran '...  Entertainment  \n",
      "2   the u.s. fish and wildlife service has reopen...     Technology  \n",
      "3   the very nature of social media means it is o...     Technology  \n",
      "4   caesars plans us$880 mln new york casino jul ...       Business  \n",
      "\n",
      "Data summary:\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 111795 entries, 0 to 111794\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count   Dtype\n",
      "---  ------   --------------   -----\n",
      " 0   Id       111795 non-null  int64\n",
      " 1   Title    111795 non-null  str  \n",
      " 2   Content  111795 non-null  str  \n",
      " 3   Label    111795 non-null  str  \n",
      "dtypes: int64(1), str(3)\n",
      "memory usage: 3.4 MB\n",
      "None\n",
      "\n",
      "Missing values in each column:\n",
      "Id         0\n",
      "Title      0\n",
      "Content    0\n",
      "Label      0\n",
      "dtype: int64\n",
      "\n",
      "Column data types:\n",
      "Id         int64\n",
      "Title        str\n",
      "Content      str\n",
      "Label        str\n",
      "dtype: object\n",
      "dataTrain shape: (111795, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = 'bigdata2025classification/train.csv'\n",
    "\n",
    "def load_and_process_data(file_path):\n",
    "    # Load data from a CSV file\n",
    "    dataTrain = pd.read_csv(file_path)\n",
    "\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(\"First 5 rows of the dataset:\")\n",
    "    print(dataTrain.head())\n",
    "\n",
    "    print(\"\\nData summary:\")\n",
    "    print(dataTrain.info())\n",
    "\n",
    "    # Check for missing values in the dataframe\n",
    "    print(\"\\nMissing values in each column:\")\n",
    "    print(dataTrain.isnull().sum())\n",
    "    \n",
    "    return dataTrain\n",
    "\n",
    "dataTrain = load_and_process_data(file_path)\n",
    "\n",
    "# check column data types\n",
    "def check_column_types(dataTrain):\n",
    "    print(\"\\nColumn data types:\")\n",
    "    print(dataTrain.dtypes)\n",
    "\n",
    "check_column_types(dataTrain)\n",
    "\n",
    "\n",
    "print(f\"dataTrain shape: {dataTrain.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726323d",
   "metadata": {},
   "source": [
    "## Keeping 10000 instances to reduce computation load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3cce109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset shape: (15000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Keep only the first 15000 instances for faster experimentation\n",
    "dataTrain = dataTrain.iloc[:15000].reset_index(drop=True)\n",
    "print(f\"Subset shape: {dataTrain.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b81952",
   "metadata": {},
   "source": [
    "## Duplicate removal based on Title and Content columns concurently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1deeda1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicates based on Title and Content removed. Data shape: (14988, 4)\n",
      "\n",
      "Index reset. Data shape: (14988, 4)\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 14988 entries, 0 to 14987\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   Id       14988 non-null  int64\n",
      " 1   Title    14988 non-null  str  \n",
      " 2   Content  14988 non-null  str  \n",
      " 3   Label    14988 non-null  str  \n",
      "dtypes: int64(1), str(3)\n",
      "memory usage: 468.5 KB\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates based on 'Title' and 'Content' columns, keeping the first occurrence\n",
    "dataTrain = dataTrain.drop_duplicates(subset=['Title', 'Content'], keep='first')\n",
    "print(\"\\nDuplicates based on Title and Content removed. Data shape:\", dataTrain.shape)\n",
    "\n",
    "\n",
    "# Reset the index after removing duplicates\n",
    "dataTrain = dataTrain.reset_index(drop=True)\n",
    "print(\"\\nIndex reset. Data shape:\", dataTrain.shape)\n",
    "dataTrain.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f3320",
   "metadata": {},
   "source": [
    "\n",
    "### Data statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f964a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Content Statistics ---\n",
      "        text_length    word_count  sentence_count  avg_word_length\n",
      "count  14988.000000  14988.000000    14988.000000     14988.000000\n",
      "mean    2561.735722    422.912797       24.663531         5.071503\n",
      "std     2196.271772    364.721403       25.236777         0.825487\n",
      "min       17.000000      3.000000        1.000000         3.500000\n",
      "25%     1302.750000    216.000000       12.000000         4.835203\n",
      "50%     2025.000000    336.000000       19.000000         5.054566\n",
      "75%     3142.000000    515.000000       30.000000         5.268253\n",
      "max    78614.000000  12562.000000     1343.000000        95.820513\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dataTrain['text_length'] = dataTrain['Content'].apply(len)\n",
    "dataTrain['word_count'] = dataTrain['Content'].apply(lambda x: len(str(x).split()))\n",
    "dataTrain['sentence_count'] = dataTrain['Content'].apply(lambda x: len(str(x).split('.')))\n",
    "dataTrain['avg_word_length'] = dataTrain['Content'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))\n",
    "\n",
    "print(\"\\n--- Content Statistics ---\")\n",
    "print(dataTrain[['text_length', 'word_count', 'sentence_count', 'avg_word_length']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a364cf5",
   "metadata": {},
   "source": [
    "\n",
    "### Remove words not in English dictionary\n",
    "\n",
    "- **probably could change dictionary for better results but it works...**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc5b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Download the words corpus if not already present\n",
    "nltk.download('words')\n",
    "english_words = set(words.words())\n",
    "\n",
    "def remove_non_english_words(text):\n",
    "    # Split text into words\n",
    "    word_list = re.findall(r'\\b\\w+\\b', str(text))\n",
    "    cleaned_words = []\n",
    "    for word in word_list:\n",
    "        # Drop any word not in dictionary\n",
    "        if word.lower() not in english_words:\n",
    "            continue\n",
    "        # Drop words with 2+ repeating chars not in dictionary (redundant now, but kept for clarity)\n",
    "        if re.search(r'(.)\\1{1,}', word):\n",
    "            if word.lower() not in english_words:\n",
    "                continue\n",
    "        cleaned_words.append(word)\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Apply to both columns\n",
    "dataTrain['Title'] = dataTrain['Title'].apply(remove_non_english_words)\n",
    "dataTrain['Content'] = dataTrain['Content'].apply(remove_non_english_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec666e",
   "metadata": {},
   "source": [
    "## Text clean up \n",
    "1. Expand contractions\n",
    "2. Convert to lowercase\n",
    "3. Remove special characters (keep only letters and spaces)\n",
    "4. Remove extra spaces\n",
    "5. Remove stopwords, lemmatize, and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac4b0184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\odys_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download required NLTK data if not already present\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "    # Remove stopwords, lemmatize, and stem\n",
    "    words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words if word not in stop_words]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "for col in ['Title', 'Content']:\n",
    "    dataTrain[col] = dataTrain[col].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888cc01",
   "metadata": {},
   "source": [
    "### Just printing out the firtst 5 columns to see what happend to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68cb3ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id                       Title  \\\n",
      "0  227464  come cabl groceri overlord   \n",
      "1  244074          presid react happi   \n",
      "2   60707              wildlif servic   \n",
      "3   27883                      launch   \n",
      "4  169596           u new york casino   \n",
      "\n",
      "                                             Content          Label  \\\n",
      "0  subscrib one three dink compar speak cabl abl ...  Entertainment   \n",
      "1  presid react happi singer presid took twitter ...  Entertainment   \n",
      "2  fish wildlif servic comment period addit day p...     Technology   \n",
      "3  natur social medium often sourc real time brea...     Technology   \n",
      "4  u new york casino latest news top deck world e...       Business   \n",
      "\n",
      "   text_length  word_count  sentence_count  avg_word_length  \n",
      "0         1576         264              15         4.965909  \n",
      "1         1200         192              10         5.250000  \n",
      "2         2773         416              34         5.665865  \n",
      "3         1564         254              13         5.157480  \n",
      "4         2250         365              23         5.164384  \n"
     ]
    }
   ],
   "source": [
    "print(dataTrain.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d8862",
   "metadata": {},
   "source": [
    "## Starting future extraction (converting text to numbers for ML algorythms to run)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff593d17",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization\n",
    "\n",
    "We will now use TF-IDF vectorization instead of Bag of Words to represent the text data for classification. TF-IDF often improves performance by reducing the impact of common words and highlighting more informative terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f6b4208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (14988, 10000)\n",
      "Feature names (first 20): ['aa' 'abandon' 'abdomin' 'abid' 'abil' 'abl' 'abl access' 'abl find'\n",
      " 'abl get' 'abl make' 'abl see' 'abl take' 'abl use' 'abnorm' 'aboard'\n",
      " 'abort' 'abroad' 'abrupt' 'abruptli' 'absenc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine Title and Content if not already done\n",
    "if 'Combined' not in dataTrain.columns:\n",
    "    dataTrain['Combined'] = dataTrain['Title'].fillna('') + ' ' + dataTrain['Content'].fillna('')\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "# You can tune max_features, ngram_range, etc. for further improvement\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=10000)\n",
    "dataTrain_tfidf = vectorizer.fit_transform(dataTrain['Combined'])\n",
    "\n",
    "print('TF-IDF matrix shape:', dataTrain_tfidf.shape)\n",
    "print('Feature names (first 20):', vectorizer.get_feature_names_out()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8641549a",
   "metadata": {},
   "source": [
    "### Logistic Regression with Built-in Cross-Validation (LogisticRegressionCV)\n",
    "\n",
    "We will now use `LogisticRegressionCV` from scikit-learn, which performs cross-validated logistic regression and automatically tunes the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fe1db77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github\\bigData\\venvEnv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1780: FutureWarning: The default value for l1_ratios will change from None to (0.0,) in version 1.10. From version 1.10 onwards, only array-like with values in [0, 1] will be allowed, None will be forbidden. To avoid this warning, explicitly set a value, e.g. l1_ratios=(0,).\n",
      "  warnings.warn(\n",
      "d:\\Github\\bigData\\venvEnv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1823: FutureWarning: The fitted attributes of LogisticRegressionCV will be simplified in scikit-learn 1.10 to remove redundancy. Set`use_legacy_attributes=False` to enable the new behavior now, or set it to `True` to silence this warning during the transition period while keeping the deprecated behavior for the time being. The default value of use_legacy_attributes will change from True to False in scikit-learn 1.10. See the docstring of LogisticRegressionCV for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C values per class: [21.5443469 21.5443469 21.5443469 21.5443469]\n",
      "\n",
      " ***********************\n",
      "\n",
      "Classification Report (LogisticRegressionCV, 5-fold CV):\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     Business       0.99      1.00      1.00      3372\n",
      "Entertainment       1.00      1.00      1.00      5949\n",
      "       Health       1.00      1.00      1.00      1610\n",
      "   Technology       1.00      1.00      1.00      4057\n",
      "\n",
      "     accuracy                           1.00     14988\n",
      "    macro avg       1.00      1.00      1.00     14988\n",
      " weighted avg       1.00      1.00      1.00     14988\n",
      "\n",
      "\n",
      " classification accuracy a= 0.998065118761676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assume the target column is named 'Label' (change if needed)\n",
    "if 'Label' not in dataTrain.columns:\n",
    "    print(\"ERROR: 'Label' column not found in dataTrain. Please check your dataset.\")\n",
    "else:\n",
    "    X = dataTrain_tfidf\n",
    "    y = dataTrain['Label']\n",
    "    clf_cv = LogisticRegressionCV(cv=5, max_iter=1000, random_state=42,class_weight='balanced',scoring='accuracy', n_jobs= 6)\n",
    "    clf_cv.fit(X, y)\n",
    "    y_pred_cv = clf_cv.predict(X)\n",
    "    print(\"Best C values per class:\", clf_cv.C_)\n",
    "    print('\\n ***********************')\n",
    "    \n",
    "\n",
    "    print(\"\\nClassification Report (LogisticRegressionCV, 5-fold CV):\\n\", classification_report(y, y_pred_cv, zero_division=0))\n",
    "    \n",
    "    print ('\\n classification accuracy a=', clf_cv.score(X, y))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064ddb3",
   "metadata": {},
   "source": [
    "## Writing test predictions to file testSet_categories.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68bafee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load test data\n",
    "# test_file_path = 'bigdata2025classification/test_without_labels.csv'\n",
    "# test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "# # Apply the same text preprocessing to the test data\n",
    "\n",
    "# # Remove non-English words\n",
    "# test_data['Title'] = test_data['Title'].apply(remove_non_english_words)\n",
    "# test_data['Content'] = test_data['Content'].apply(remove_non_english_words)\n",
    "\n",
    "# # Clean text (expand contractions, lowercase, remove special chars, stopwords, lemmatize, stem)\n",
    "# for col in ['Title', 'Content']:\n",
    "#     test_data[col] = test_data[col].astype(str).apply(clean_text)\n",
    "\n",
    "# # Combine Title and Content for test data (same as training)\n",
    "# test_data['Combined'] = test_data['Title'].fillna('') + ' ' + test_data['Content'].fillna('')\n",
    "\n",
    "# # Apply the same TF-IDF vectorizer to test data\n",
    "# test_tfidf = vectorizer.transform(test_data['Combined'])\n",
    "\n",
    "# # Predict labels using the trained classifier\n",
    "# test_pred = clf_cv.predict(test_tfidf)\n",
    "\n",
    "# # Prepare output DataFrame\n",
    "# output_df = pd.DataFrame({\n",
    "#     'Id': test_data['Id'],\n",
    "#     'Predicted': test_pred\n",
    "# })\n",
    "\n",
    "# # Write predictions to CSV\n",
    "# output_df.to_csv('testSet_categories.csv', index=False)\n",
    "# print(\"Predictions written to testSet_categories.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvEnv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
