{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50153835",
   "metadata": {},
   "source": [
    "# M161 first question notebook- best model LogisticRegrgessionCV , 0.9929 accuracy, 111220 (all) instances, max_features=35000 vectorization, n_gram (1,2)\n",
    "\n",
    "### Data from D:\\Github\\bigData\\part1\\joblibCache\\dataTrain_cleaned.joblib\n",
    "(duplicate removal and text processed already including stemming an d lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff19713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>227464</td>\n",
       "      <td>come cabl groceri overlord</td>\n",
       "      <td>subscrib one three dink compar speak cabl abl ...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>244074</td>\n",
       "      <td>presid react happi</td>\n",
       "      <td>presid react happi singer presid took twitter ...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60707</td>\n",
       "      <td>wildlif servic</td>\n",
       "      <td>fish wildlif servic comment period addit day p...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27883</td>\n",
       "      <td>launch</td>\n",
       "      <td>natur social medium often sourc real time brea...</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169596</td>\n",
       "      <td>u new york casino</td>\n",
       "      <td>u new york casino latest news top deck world e...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                       Title  \\\n",
       "0  227464  come cabl groceri overlord   \n",
       "1  244074          presid react happi   \n",
       "2   60707              wildlif servic   \n",
       "3   27883                      launch   \n",
       "4  169596           u new york casino   \n",
       "\n",
       "                                             Content          Label  \n",
       "0  subscrib one three dink compar speak cabl abl ...  Entertainment  \n",
       "1  presid react happi singer presid took twitter ...  Entertainment  \n",
       "2  fish wildlif servic comment period addit day p...     Technology  \n",
       "3  natur social medium often sourc real time brea...     Technology  \n",
       "4  u new york casino latest news top deck world e...       Business  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "dataTrain = joblib.load(r'joblibCache\\dataTrain_cleaned.joblib')\n",
    "dataTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f50c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stratify and keep 50000 instances based on the 'Label' column\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Stratify and sample 50000 instances\n",
    "# stratified_data, _ = train_test_split(\n",
    "#     dataTrain,\n",
    "#     train_size=50000,\n",
    "#     stratify=dataTrain['Label'],\n",
    "#     random_state=42\n",
    "# )\n",
    "# dataTrain = stratified_data.reset_index(drop=True)\n",
    "# print(f\"Subset shape (stratified): {dataTrain.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888cc01",
   "metadata": {},
   "source": [
    "### Just printing out the firtst 5 columns to see what happend to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68cb3ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 111220 entries, 0 to 111219\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count   Dtype\n",
      "---  ------   --------------   -----\n",
      " 0   Id       111220 non-null  int64\n",
      " 1   Title    111220 non-null  str  \n",
      " 2   Content  111220 non-null  str  \n",
      " 3   Label    111220 non-null  str  \n",
      "dtypes: int64(1), str(3)\n",
      "memory usage: 3.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dataTrain.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d8862",
   "metadata": {},
   "source": [
    "## Starting future extraction (converting text to numbers for ML algorythms to run)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff593d17",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization\n",
    "\n",
    "We will now use TF-IDF vectorization instead of Bag of Words to represent the text data for classification. TF-IDF often improves performance by reducing the impact of common words and highlighting more informative terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f6b4208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (111220, 35000)\n",
      "Feature names (first 20): ['aa' 'aback' 'abandon' 'abbey' 'abdomen' 'abdomin' 'abdomin pain' 'abid'\n",
      " 'abil' 'abil make' 'abil use' 'abl' 'abl access' 'abl afford' 'abl bring'\n",
      " 'abl buy' 'abl captur' 'abl continu' 'abl control' 'abl creat']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine Title and Content if not already done\n",
    "if 'Combined' not in dataTrain.columns:\n",
    "    dataTrain['Combined'] = dataTrain['Title'].fillna('') + ' ' + dataTrain['Content'].fillna('')\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "# You can tune max_features, ngram_range, etc. for further improvement\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=35000)\n",
    "dataTrain_tfidf = vectorizer.fit_transform(dataTrain['Combined'])\n",
    "\n",
    "print('TF-IDF matrix shape:', dataTrain_tfidf.shape)\n",
    "print('Feature names (first 20):', vectorizer.get_feature_names_out()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8641549a",
   "metadata": {},
   "source": [
    "### Logistic Regression with Built-in Cross-Validation (LogisticRegressionCV)\n",
    "\n",
    "We will now use `LogisticRegressionCV` from scikit-learn, which performs cross-validated logistic regression and automatically tunes the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe1db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegressionCV\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Assume the target column is named 'Label' (change if needed)\n",
    "# if 'Label' not in dataTrain.columns:\n",
    "#     print(\"ERROR: 'Label' column not found in dataTrain. Please check your dataset.\")\n",
    "# else:\n",
    "#     X = dataTrain_tfidf\n",
    "#     y = dataTrain['Label']\n",
    "#     clf_cv = LogisticRegressionCV(cv=5, max_iter=1000, random_state=42,class_weight='balanced',scoring='accuracy', n_jobs= 6)\n",
    "#     clf_cv.fit(X, y)\n",
    "#     y_pred_cv = clf_cv.predict(X)\n",
    "#     print(\"Best C values per class:\", clf_cv.C_)\n",
    "#     print('\\n ***********************')\n",
    "    \n",
    "\n",
    "#     print(\"\\nClassification Report (LogisticRegressionCV, 5-fold CV):\\n\", classification_report(y, y_pred_cv, zero_division=0))\n",
    "    \n",
    "#     print ('\\n classification accuracy a=', clf_cv.score(X, y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d4b3147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold CV Accuracy Scores: [0.96241683 0.95922496 0.95846071 0.96111311 0.96016903]\n",
      "Mean CV Accuracy: 0.9602769286099623\n",
      "Std CV Accuracy: 0.0013925026294840796\n"
     ]
    }
   ],
   "source": [
    "# 5-Fold Cross-Validation Results for Classifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "if 'Label' not in dataTrain.columns:\n",
    "    print(\"ERROR: 'Label' column not found in dataTrain. Please check your dataset.\")\n",
    "else:\n",
    "    X = dataTrain_tfidf\n",
    "    y = dataTrain['Label']\n",
    "    clf = LogisticRegressionCV(cv=5, max_iter=1000, random_state=42, scoring='accuracy', n_jobs=6)\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy', n_jobs=6)\n",
    "    print(\"5-Fold CV Accuracy Scores:\", scores)\n",
    "    print(\"Mean CV Accuracy:\", np.mean(scores))\n",
    "    print(\"Std CV Accuracy:\", np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03b02f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [111220, 50000, 111220]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Display Confusion Matrix for LogisticRegressionCV\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, ConfusionMatrixDisplay\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m cm = \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataTrain\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_cv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m ConfusionMatrixDisplay(cm).plot()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github\\bigData\\venvEnv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github\\bigData\\venvEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:549\u001b[39m, in \u001b[36mconfusion_matrix\u001b[39m\u001b[34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[39m\n\u001b[32m    546\u001b[39m     sample_weight = _convert_to_numpy(sample_weight, xp)\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sample_weight) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m     y_type, y_true, y_pred, sample_weight = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    553\u001b[39m     \u001b[38;5;66;03m# This is needed to handle the special case where y_true, y_pred and\u001b[39;00m\n\u001b[32m    554\u001b[39m     \u001b[38;5;66;03m# sample_weight are all empty.\u001b[39;00m\n\u001b[32m    555\u001b[39m     \u001b[38;5;66;03m# In this case we don't pass sample_weight to _check_targets that would\u001b[39;00m\n\u001b[32m    556\u001b[39m     \u001b[38;5;66;03m# check that sample_weight is not empty and we don't reuse the returned\u001b[39;00m\n\u001b[32m    557\u001b[39m     \u001b[38;5;66;03m# sample_weight\u001b[39;00m\n\u001b[32m    558\u001b[39m     y_type, y_true, y_pred, _ = _check_targets(y_true, y_pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github\\bigData\\venvEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:108\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred, sample_weight)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[32m     78\u001b[39m \n\u001b[32m     79\u001b[39m \u001b[33;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \u001b[33;03msample_weight : array or None\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    107\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m type_true = type_of_target(y_true, input_name=\u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m type_pred = type_of_target(y_pred, input_name=\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github\\bigData\\venvEnv\\Lib\\site-packages\\sklearn\\utils\\validation.py:464\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    462\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    465\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    466\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    467\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [111220, 50000, 111220]"
     ]
    }
   ],
   "source": [
    "# # Display Confusion Matrix for LogisticRegressionCV\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# cm = confusion_matrix(dataTrain['Label'], y_pred_cv)\n",
    "# ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064ddb3",
   "metadata": {},
   "source": [
    "## Writing test predictions to file testSet_categories.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bafee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions written to testSet_categoriesLogReg.csv\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import joblib \n",
    "\n",
    "# test_data = joblib.load(r'joblibCache\\dataTest_cleaned.joblib')\n",
    "# test_data.head()\n",
    "\n",
    "\n",
    "# # Combine Title and Content for test data (same as training)\n",
    "# test_data['Combined'] = test_data['Title'].fillna('') + ' ' + test_data['Content'].fillna('')\n",
    "\n",
    "# # Apply the same TF-IDF vectorizer to test data\n",
    "# test_tfidf = vectorizer.transform(test_data['Combined'])\n",
    "\n",
    "# # Predict labels using the trained classifier\n",
    "# test_pred = clf_cv.predict(test_tfidf)\n",
    "\n",
    "# # Prepare output DataFrame\n",
    "# output_df = pd.DataFrame({\n",
    "#     'Id': test_data['Id'],\n",
    "#     'Predicted': test_pred\n",
    "# })\n",
    "\n",
    "# # Write predictions to CSV\n",
    "# output_df.to_csv('testSet_categoriesLogReg.csv', index=False)\n",
    "# print(\"Predictions written to testSet_categoriesLogReg.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvEnv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
